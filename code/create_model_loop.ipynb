{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load data and define transforms\n",
    "\n",
    "imagenet_means = [0.485, 0.456, 0.406]\n",
    "imagenet_stds = [0.229, 0.224, 0.225]\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Scale(size = 256),\n",
    "        transforms.RandomSizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(imagenet_means, imagenet_stds)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Scale(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(imagenet_means, imagenet_stds)\n",
    "    ]),\n",
    "    'test_kaggle': transforms.Compose([\n",
    "        transforms.Scale(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(imagenet_means, imagenet_stds)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = '../../data/all_data_scaled_alt' # directory containing the data\n",
    "\n",
    "dsets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "         for x in ['train', 'val']}\n",
    "\n",
    "dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size = 64,\n",
    "                                               shuffle=True, num_workers = 8, pin_memory = True)\n",
    "                for x in ['train', 'val']}\n",
    "\n",
    "dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "dset_classes = dsets['train'].classes\n",
    "\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "exp_str = 'phase1_submission' # labels the directory where figures, kaggle predictions, and model checkpoints will be saved\n",
    "pretrained = True\n",
    "augmentations = 'basic'\n",
    "# Define hyperparameters used to generate the models used in the ensemble\n",
    "# num_epochs_list = [12, 17, 22, 16, 7]\n",
    "num_epochs_list = [40, 40, 40, 40, 40]\n",
    "lr_list = [1.65e-4, 5.96e-4, 1.21e-4, 3.86e-4, 1.55e-4]\n",
    "weight_decay_list = [3.17e-8, 1.99e-6, 2.08e-2, 8,70e-5, 3.17e-2]\n",
    "lr_decay_epochs_list = [12, 8, 11, 12, 12]\n",
    "lr_decay_const_list = [0.3, 0.5, 0.4, 0.7, 0.6]\n",
    "model_name_list = ['resnet18', 'resnet18', 'resnet18', 'resnet34', 'resnet34']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/39\n",
      "----------\n",
      "LR is set to 0.000165\n",
      "train Loss: 0.9705 Acc: 0.5316\n",
      "Type_1 F1: 0.2631 Precision: 0.3765, Recall: 0.2022, Specificity: 0.9286\n",
      "Type_2 F1: 0.6473 Precision: 0.5708, Recall: 0.7475, Specificity: 0.3679\n",
      "Type_3 F1: 0.3958 Precision: 0.4727, Recall: 0.3405, Specificity: 0.8411\n",
      "val Loss: 0.9838 Acc: 0.5811\n",
      "Type_1 F1: 0.1034 Precision: 0.3750, Recall: 0.0600, Specificity: 0.9797\n",
      "Type_2 F1: 0.7000 Precision: 0.5738, Recall: 0.8974, Specificity: 0.2571\n",
      "Type_3 F1: 0.4328 Precision: 0.6591, Recall: 0.3222, Specificity: 0.9272\n",
      "new best model\n",
      "Epoch 1/39\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "performance_df_list = []\n",
    "best_model_list = []\n",
    "for i in range(len(num_epochs_list)):\n",
    "    ## Define hyperparameters\n",
    "    num_epochs = num_epochs_list[i]\n",
    "    lr = lr_list[i]\n",
    "    weight_decay = weight_decay_list[i]\n",
    "\n",
    "    # For learning rate annealing\n",
    "    lr_decay_epochs = lr_decay_epochs_list[i]\n",
    "    lr_decay_const = lr_decay_const_list[i]\n",
    "    \n",
    "    model_name = model_name_list[i]\n",
    "    \n",
    "    # For naming files\n",
    "    description_string = model_name + 'lr' + str(lr) + \\\n",
    "                            '_wd' + str(weight_decay) + \\\n",
    "                            '_decayep' + str(lr_decay_epochs) + \\\n",
    "                            '_decayconst' + str(lr_decay_const) + \\\n",
    "                            'stamp_' + str(time.time())\n",
    "\n",
    "    # For passing to functions                \n",
    "    hyperparam_dict = {'num_epochs':num_epochs,\n",
    "                       'lr':lr,\n",
    "                       'weight_decay':weight_decay,\n",
    "                       'lr_decay_epochs':lr_decay_epochs,\n",
    "                       'lr_decay_const':lr_decay_const,\n",
    "                       'pretrained':str(pretrained),\n",
    "                       'model_name':model_name,\n",
    "                       'description':description_string,\n",
    "                       'augmentations':augmentations\n",
    "                       }\n",
    "    \n",
    "    ## Fine tuning conv-net\n",
    "    if model_name == 'resnet18':\n",
    "        model_ft = models.resnet18(pretrained=pretrained)\n",
    "    elif model_name == 'resnet34':\n",
    "        model_ft = models.resnet34(pretrained=pretrained)\n",
    "        \n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, 3)\n",
    "    model_ft = torch.nn.DataParallel(model_ft).cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_ft = optim.Adam(model_ft.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "    \n",
    "    from PIL import ImageFile, Image\n",
    "    ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "    scheduler = exp_lr_scheduler\n",
    "\n",
    "    model_ft, performance_dict = train_model(model_ft, \n",
    "                                             criterion, \n",
    "                                             optimizer_ft, \n",
    "                                             scheduler, \n",
    "                                             dset_loaders = dset_loaders, \n",
    "                                             dset_sizes = dset_sizes, \n",
    "                                             num_epochs = num_epochs, \n",
    "                                             init_lr = lr,\n",
    "                                             lr_decay_epochs = lr_decay_epochs,\n",
    "                                             lr_decay_const = lr_decay_const)\n",
    "    best_model_list.append(model_ft)\n",
    "    \n",
    "    ## Save the model weights\n",
    "    model_dir = '../model_weights/experiments/'\n",
    "    maybe_makedir(model_dir)\n",
    "    model_dir = model_dir + exp_str + '/'\n",
    "    maybe_makedir(model_dir)\n",
    "    \n",
    "    torch.save(model_ft.state_dict(), model_dir + description_string + '.chkpt')\n",
    "    \n",
    "    # Convert performance to a dataframe\n",
    "    performance_df = convert_performance_dict(performance_dict, hyperparam_dict)\n",
    "    performance_df_list.append(performance_df)\n",
    "    \n",
    "    # Save performance results\n",
    "    performance_path = '../performance/'\n",
    "    maybe_makedir(performance_path)\n",
    "    performance_path = performance_path + exp_str + '/'\n",
    "    maybe_makedir(performance_path)\n",
    "    performance_df.to_csv(path_or_buf = performance_path + description_string + '.csv', index = False)\n",
    "    \n",
    "    # # Kaggle Test Set Predictions\n",
    "    data_transforms = {\n",
    "        'test_kaggle': transforms.Compose([\n",
    "            transforms.Scale(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(imagenet_means, imagenet_stds)\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    # Kaggle Test Set Predictions\n",
    "\n",
    "    dsets_kaggle = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "             for x in ['test_kaggle']}\n",
    "\n",
    "    dset_loaders_kaggle = {x: torch.utils.data.DataLoader(dsets_kaggle[x], batch_size = 64,\n",
    "                                                   shuffle=False, num_workers = 8, pin_memory = True)\n",
    "                    for x in ['test_kaggle']}\n",
    "    \n",
    "    dset_classes_kaggle = dsets_kaggle['test_kaggle'].classes\n",
    "    \n",
    "    phase = 'test_kaggle'\n",
    "    test_loader = dset_loaders_kaggle[phase]\n",
    "\n",
    "    file_names = test_loader.dataset.imgs\n",
    "    # Extract just the short file name\n",
    "    file_names = [x[0].split('/')[-1] for x in file_names]\n",
    "\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "\n",
    "    probs = predict_on_test(model_ft, test_loader, dtype)\n",
    "    import pandas as pd\n",
    "    predictions_df = pd.DataFrame(data = probs, columns = (dset_classes_kaggle))\n",
    "\n",
    "    predictions_df.insert(0, column = 'image_name', value = file_names)\n",
    "    print(predictions_df)\n",
    "    predictions_dir = '../test_predictions/' + 'experiments' + '/'\n",
    "    maybe_makedir(predictions_dir)\n",
    "    predictions_dir = predictions_dir + exp_str + '/'\n",
    "    maybe_makedir(predictions_dir)\n",
    "    predictions_df.to_csv(path_or_buf = predictions_dir + description_string + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine the performance dicts together\n",
    "performance_df_all = pd.concat(performance_df_list)\n",
    "performance_df_all.to_csv(performance_path + exp_str + '_all.csv', index=False)\n",
    "# print(performance_df_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
